{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "class SquareImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        img = torch.zeros(1, 32, 32, dtype=torch.float32)\n",
    "        img[:, 8:24, 8:24] = 1.0\n",
    "        self.img = img\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img, 0\n",
    "\n",
    "train_dataset = SquareImageDataset()\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=1)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from einops import repeat\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "class FlowMatchingModel(nn.Module):\n",
    "    \"Flow-matching CNN written with a single nn.Sequential block.\"\n",
    "    def __init__(self, img_size: int = 32, kernel_size: int = 3, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=hidden_dim, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=1, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x_t: Tensor, t: Tensor) -> Tensor:\n",
    "        if t.ndim == 0:\n",
    "            t = t.view(1).repeat(x_t.size(0))\n",
    "        if t.ndim == 1:\n",
    "            t = t.view(x_t.size(0), 1, 1, 1)\n",
    "        elif t.ndim != 4:\n",
    "            raise ValueError(\"t must be of shape (), (B,) or (B,1,H,W)\")\n",
    "        t = t.expand(-1, 1, x_t.size(2), x_t.size(3))\n",
    "        x = torch.cat([x_t, t], dim=1)\n",
    "        return self.net(x)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.optim import AdamW\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n",
    "\n",
    "path = AffineProbPath(scheduler=CondOTScheduler())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "fm = FlowMatchingModel(img_size=32, kernel_size=3).to(device)\n",
    "optim = AdamW(params=fm.parameters(), lr=5e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    fm.train()\n",
    "    for batch in train_loader:\n",
    "        x_1 = batch[0].to(device)\n",
    "        x_0 = torch.randn_like(x_1)\n",
    "        B, C, H, W = x_0.shape\n",
    "        t = torch.rand(B, 1, 1, 1, device=device)\n",
    "        path_sample = path.sample(t=t.squeeze(), x_0=x_0, x_1=x_1)\n",
    "        x_t = path_sample.x_t\n",
    "        dx_t = path_sample.dx_t\n",
    "        pred = fm(x_t, t)\n",
    "        loss = loss_fn(pred, dx_t)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}: Loss = {loss.item():.5f}')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras):\n",
    "        return self.model(x, t)\n",
    "\n",
    "wrapped_vf = WrappedModel(fm)\n",
    "\n",
    "number_of_steps = 500\n",
    "step_size = 1 / (number_of_steps * 3)\n",
    "T = torch.linspace(0, 1, number_of_steps).to(device)\n",
    "\n",
    "x_init = torch.randn((1, 1, 32, 32), device=device)\n",
    "solver = ODESolver(velocity_model=wrapped_vf)\n",
    "sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_image_evolution(sol, number_of_steps, n_plots=10, title_prefix='Step'):\n",
    "    images = sol[:number_of_steps, 0]\n",
    "    plot_indices = np.linspace(0, number_of_steps-1, n_plots, dtype=int)\n",
    "    plt.figure(figsize=(2.5*n_plots, 3))\n",
    "    for j, i in enumerate(plot_indices):\n",
    "        img = images[i].detach().cpu().squeeze().numpy()\n",
    "        plt.subplot(1, n_plots, j+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'{title_prefix} {i}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_image_evolution(sol, number_of_steps, n_plots=10)\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow_matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}