{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    def __init__(self, dim: int = 1, h: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, dim))\n",
    "\n",
    "    def forward(self, t: Tensor, x_t: Tensor) -> Tensor:\n",
    "        return self.net(torch.cat((t, x_t), -1))\n",
    "\n",
    "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor) -> Tensor:\n",
    "        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n",
    "        return x_t + (t_end - t_start) * self(\n",
    "            t=t_start + (t_end - t_start) / 2,\n",
    "            x_t=x_t + self(x_t=x_t, t=t_start) * (t_end - t_start) / 2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "samples = torch.randn(1000, 1)\n",
    "plt.hist(samples.numpy(), bins=30, density=True)\n",
    "plt.title(\"Target distribution $\\mathcal{N}(0, 1)$\")\n",
    "plt.xlim(-3, 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "flow = Flow()\n",
    "\n",
    "optimizer = torch.optim.Adam(flow.parameters(), 1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for _ in range(10000):\n",
    "    x_1 = torch.randn(256, 1)\n",
    "    x_0 = torch.randn_like(x_1)\n",
    "    t = torch.rand(len(x_1), 1)\n",
    "\n",
    "    x_t = (1 - t) * x_0 + t * x_1\n",
    "    dx_t = x_1 - x_0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_fn(flow(t=t, x_t=x_t), dx_t).backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = torch.randn(300, 1)\n",
    "n_steps = 8\n",
    "fig, axes = plt.subplots(1, n_steps + 1, figsize=(20, 3), sharey=True)\n",
    "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
    "\n",
    "axes[0].hist(x.detach().numpy(), bins=30, range=(-3, 3))\n",
    "axes[0].set_title(f\"t = {time_steps[0]:.2f}\")\n",
    "axes[0].set_xlim(-3.0, 3.0)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1])\n",
    "    axes[i + 1].hist(x.detach().numpy(), bins=30, range=(-3, 3))\n",
    "    axes[i + 1].set_title(f\"t = {time_steps[i + 1]:.2f}\")\n",
    "    axes[i + 1].set_xlim(-3.0, 3.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}